<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Election Integrity MAS - System Documentation</title>
    <style>
        :root {
            --bg-color: #525659;
            --paper-color: #ffffff;
            --text-primary: #333333;
            --text-secondary: #555555;
            --accent-color: #2563eb; /* Royal Blue */
            --code-bg: #f5f7fa;
            --border-color: #e5e7eb;
        }

        body {
            background-color: var(--bg-color);
            margin: 0;
            padding: 40px 20px;
            font-family: 'Segoe UI', 'Roboto', Helvetica, Arial, sans-serif;
            color: var(--text-primary);
            display: flex;
            flex-direction: column;
            align-items: center;
            min-height: 100vh;
        }

        /* PDF Page Simulation */
        .pdf-page {
            background-color: var(--paper-color);
            width: 100%;
            max-width: 850px; /* A4 width approx */
            min-height: 1100px; /* A4 height approx */
            padding: 60px 80px;
            box-shadow: 0 10px 25px rgba(0,0,0,0.5);
            margin-bottom: 30px;
            position: relative;
            box-sizing: border-box;
        }

        /* Typography */
        h1 {
            font-size: 28px;
            border-bottom: 3px solid var(--accent-color);
            padding-bottom: 15px;
            margin-bottom: 40px;
            color: #1a1a1a;
            text-transform: uppercase;
            letter-spacing: 1px;
        }

        h2 {
            font-size: 20px;
            color: var(--accent-color);
            margin-top: 35px;
            margin-bottom: 15px;
            font-weight: 700;
            display: flex;
            align-items: center;
        }

        /* Numbered Badges for Sections */
        h2::before {
            content: attr(data-num);
            display: flex;
            align-items: center;
            justify-content: center;
            width: 24px;
            height: 24px;
            background-color: var(--accent-color);
            color: white;
            font-size: 12px;
            border-radius: 50%;
            margin-right: 12px;
        }

        h3 {
            font-size: 16px;
            color: #1a1a1a;
            margin-top: 20px;
            margin-bottom: 8px;
            font-weight: 600;
        }

        p {
            line-height: 1.6;
            margin-bottom: 15px;
            font-size: 15px;
            color: var(--text-primary);
            text-align: justify;
        }

        ul, ol {
            margin-bottom: 20px;
            padding-left: 20px;
        }

        li {
            margin-bottom: 8px;
            line-height: 1.6;
            font-size: 15px;
            color: var(--text-secondary);
        }

        /* Technical Highlights Box */
        .highlight-box {
            background-color: #eff6ff;
            border-left: 4px solid var(--accent-color);
            padding: 15px 20px;
            margin: 20px 0;
            border-radius: 0 4px 4px 0;
            font-size: 14px;
            color: #1e40af;
        }

        /* JSON/Code Blocks */
        pre {
            background-color: var(--code-bg);
            border: 1px solid var(--border-color);
            padding: 15px;
            border-radius: 6px;
            overflow-x: auto;
            font-family: 'Consolas', 'Monaco', monospace;
            font-size: 13px;
            color: #d63384;
            margin: 15px 0;
        }

        /* Diagram Placeholder */
        .diagram-block {
            background-color: #f8fafc;
            border: 2px dashed #cbd5e1;
            padding: 20px;
            text-align: center;
            font-family: 'Courier New', monospace;
            font-size: 13px;
            color: #475569;
            margin: 25px 0;
            border-radius: 6px;
            font-weight: bold;
        }

        .page-footer {
            margin-top: 50px;
            border-top: 1px solid #ddd;
            padding-top: 20px;
            text-align: center;
            font-size: 12px;
            color: #999;
        }

        /* Responsive */
        @media (max-width: 900px) {
            .pdf-page { width: 95%; padding: 40px; height: auto; }
        }
        @media print {
            body { background-color: white; padding: 0; }
            .pdf-page { box-shadow: none; margin: 0; width: 100%; max-width: none; }
        }
    </style>
</head>
<body>

    <div class="pdf-page">
        <h1>Election Integrity Multi-Agent System (MAS)</h1>

        <section>
            <h2 data-num="1">Problem Statement</h2>
            
            <h3>What societal issue are you addressing?</h3>
            <p>
                We are addressing the critical challenge of <strong>election misinformation</strong> and the lack of immediate, verified technical support for election officials on the ground. In high-stakes environments, rumors about EVM (Electronic Voting Machine) tampering or procedural errors—such as "171 additional votes being polled in Dima Hasao"—spread faster than official verifications.
            </p>
            
            <h3>Why does it matter?</h3>
            <p>
                Trust is the currency of democracy. When Presiding Officers cannot instantly access technical manuals, or when citizens are misled by fake images of "broken seals," confidence in the electoral process erodes. Our system bridges this gap by providing instant, manual-verified answers and visual forensics. It ensures that <strong>official truth travels as fast as the rumors</strong>, empowering officials to debunk myths like the "Bluetooth hacking" claims instantly with cited evidence.
            </p>
        </section>

        <section>
            <h2 data-num="2">System Design</h2>
            
            <h3>Architecture Overview</h3>
            <p>
                The system is architected as a stateful Multi-Agent System using <strong>LangGraph</strong>. It treats the conversation as a workflow graph rather than a simple chatbot loop.
            </p>
            <div class="diagram-block">
                [ Loader ] ➜ [ Query Generator ] ➜ [ Search Tool (Hybrid) / Visual Tool ] ➜ [ Responder ] ➜ [ Memory Scribe ]
            </div>
            <p>
                The <strong>Query Generator Node</strong> creates a list of tools to call in a strict JSON format (e.g., <code>search_image</code> for object identification or <code>search_hybrid</code> for verifying text claims). The <strong>Responder Node</strong> then synthesizes the retrieved evidence—including "Official Truth" records, debunked myths, and tweet URLs—into a coherent answer.
            </p>

            <h3>Why Qdrant is critical to your solution</h3>
            <p>
                Qdrant serves as the central integration hub of our AI, not just a storage engine. It was critical because:
            </p>
            <ul>
                <li><strong>Multi-Model Integration:</strong> It allowed us to seamlessly integrate disparate libraries—connecting <code>FastEmbed</code> for lightweight text embedding, <code>CLIP</code> for image analysis, and <code>FlashRank</code> for re-ranking results.</li>
                <li><strong>Hybrid Data Collection:</strong> We utilize Qdrant's ability to store three named vectors per point: a 768D dense vector, a sparse vector (BM25), and a 512D image vector. This allows a single database to handle keyword, semantic, and visual search simultaneously.</li>
                <li><strong>Indexed Filtering:</strong> We indexed fields like <code>Trust_score</code> and <code>Topic_Tags</code> to perform high-speed filtering (e.g., <code>"category": "Busted fake news"</code>) before the vector search even begins.</li>
            </ul>
        </section>

        <section>
            <h2 data-num="3">Multimodal Strategy</h2>
            
            <h3>What data types are used</h3>
            <p>
                The system processes a rich variety of data types to create a holistic knowledge base:
            </p>
            <ul>
                <li><strong>Official Texts:</strong> Huge extracted texts from official PDFs (Manuals, Handbooks) with proper chapter names and page numbers.</li>
                <li><strong>Disinformation Pairs:</strong> Structured "Myth vs Reality" pairs (e.g., the Assam 2021 incident clarification).</li>
                <li><strong>Visual Data:</strong> Diagrams of polling stations and images of EVM units. Crucially, we embed both the image itself <em>and</em> its long text description to allow for text-to-image search.</li>
            </ul>

            <h3>How embeddings are created and queried</h3>
            <p>
                We employ a local, multi-model embedding strategy to ensure privacy and speed:
            </p>
            <div class="highlight-box">
                <strong>1. Text:</strong> <code>intfloat/multilingual-e5-base</code> (768D) for semantic understanding of Hindi/English.<br>
                <strong>2. Images:</strong> <code>clip-ViT-B-32</code> (512D) for visual similarity (e.g., identifying a VVPAT).<br>
                <strong>3. Keywords:</strong> <code>Qdrant/bm25</code> for sparse vector generation.
            </div>
            <p>
                <strong>Semantic Chunking:</strong> Since the E5 model expects 512 tokens, we use <code>llama_index’s SemanticSplitterNodeParser</code>. This runs the embedding model to intelligently decide where to "break" a long PDF chapter so that context is preserved, rather than breaking sentences arbitrarily.
            </p>
        </section>
        
        <div class="page-footer">
            Page 1 of 2 • Election Integrity System Documentation
        </div>
    </div>

    <div class="pdf-page">
        <section>
            <h2 data-num="4">Search & Memory Logic</h2>
            
            <h3>How retrieval works</h3>
            <p>
                Our data retrieval node uses a <strong>Recursive Rank Fusion</strong> technique to combine results from two distinct search methods:
            </p>
            <ul>
                <li><strong>Dense Search:</strong> Uses brute-force nearest neighbor search on the 768D vectors. It takes more time but provides semantically correct concepts (e.g., understanding that "machine dead" means "battery failure").</li>
                <li><strong>Sparse Search:</strong> Uses keyword-based search on the sparse vectors. It is faster and ensures that specific terms like "Rule 49MA" are strictly matched.</li>
            </ul>
            <p>
                The system also adapts to user preference: if the user's interaction style is "fast", the node will prioritize Sparse Search; if "detailed", it triggers the full Dense Search.
            </p>

            <h3>How memory is stored, updated, and reused</h3>
            <p>
                Memory is stored in a dedicated <code>user_profiles</code> collection in Qdrant.
            </p>
            <ul>
                <li><strong>Storage:</strong> We do not search for users; we retrieve them using a deterministic UUID (<code>uuid5</code>) derived from their username. This creates a persistent "Psychological Profile".</li>
                <li><strong>Schema:</strong> The profile contains fields like <code>persona</code> (e.g., "Presiding Officer"), <code>interaction_style</code>, and a <code>summary</code> of the narrative.</li>
                <li><strong>Update Logic (The Scribe):</strong> After every interaction, the "Memory Update Node" reads the User Message, the Responder's Reply, and the Old Schema. It passes these to an LLM which rewrites the summary.</li>
                <li><strong>Decay:</strong> Instead of mathematical decay, we use <strong>Summarization Consolidation</strong>. Old, irrelevant details naturally drop out of the summary during the LLM's rewrite process, while core facts (Role, Location) persist.</li>
            </ul>
        </section>

        <section>
            <h2 data-num="5">Limitations & Ethics</h2>
            
            <h3>Known Failure Modes</h3>
            <p>
                The system relies heavily on the "Official Truth" records. If a specific edge case is not covered in the ECI manuals or the "Myth vs Reality" database, the agent may default to a generic "ignore_and_report" intent. Additionally, extremely low-light or blurry images may fail the CLIP analysis, leading to incorrect object identification.
            </p>

            <h3>Bias, Privacy, or Safety Considerations</h3>
            <p>
                <strong>Privacy:</strong> We prioritize user privacy by running embeddings locally. No raw user images are sent to third-party model training sets; they are vectorised in RAM and discarded immediately after the search.
            </p>
            <p>
                <strong>Safety Guardrails:</strong> The system is designed with strict `actionable_intent` categories (e.g., `educate_procedure`, `Spread_correct_info`). It is strictly prohibited from generating political opinions. If a user asks subjective questions, the `agent_guidance` field directs the AI to stick strictly to the ECI Manual's procedural text, ensuring non-partisanship.
            </p>
        </section>

        <div class="page-footer">
            Page 2 of 2 • Election Integrity System Documentation
        </div>
    </div>

</body>
</html>
